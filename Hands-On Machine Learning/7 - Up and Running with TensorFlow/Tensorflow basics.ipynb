{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Your First Graph and Running It in a Session </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um programa em TensorFlow é tipicamente separado em duas partes:\n",
    "1. Construir um gráfo de computação\n",
    "2. Executar o gráfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa biblioteca do TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criandos o grafo de computação\n",
    "x = tf.Variable(3,name=\"x\")\n",
    "y = tf.Variable(4,name=\"y\")\n",
    "f = x*x*y+y+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#A sessão é responsável por carregar o grafo em dispositivos como cpu e gpu e executá-lo, e armazena os valores das variáveis\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#Outra maneira de utilizar sessão\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    #Dessa maneira a sessão se fecha automaticamente ao final do bloco\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#Em vez de inicializar as variáveis uma por uma é possível iniciar todas de uma vez\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#InteractiveSession é uma sessão que define ela mesmo como sessão default, então não é preciso de bloco\n",
    "#Ainda é necessário fechar\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Managing Graphs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Qualquer nó que você cria é automaticamente adicionado ao gráfo default\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Você pode manipular multiplos gráfos independente, criando um novo gráfo e temporariamente fazendo default dentro de um bloco\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICA: Em Jupyter ou shell Python, é comum executar o mesmo comando várias vezes, como resultado você pode ter uma gráfo default contendo multiplos nós duplicados. Uma solução é restartar o kernel do Jupyter, mas a solução mas conveniente é ápenas resetar o gráfo default executando: tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lifecycle of a Node Value </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#Quando você avalia um nó, TensorFlow automaticamente determina os nós dependentes e automaticamente os avalia primeiramente\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # 10\n",
    "    print(z.eval()) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: Dessa maneira os valores não são reaproveitados, ou seja, quando o código acima é executado w e x são avaliados duas vezes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os nós do gráfos são deletados entre execuções de gráfos, com execção dos valores de variáveis, que são mantidos através da sessão. O ciclo de vida de uma variável começa quando executa o initializer e termina quando sessão é fechada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#Para executar o código anterior eficientemente, você pode pedir para tensorflow avaliar y e z em apenas uma execução de gráfo\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val) # 10\n",
    "    print(z_val) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Em processos singulares de TensorFlow, multiplas sessões não compartilham nenhum estado , mesmo que elas usem o mesmo  gráfo (cada sessão terá sua própria cópia de cada variável). Em TensorFlow distribuidos, o estado da variável é armazenado em servidores, não em sessões, então multiplas sessões podem compartilhar a mesma variável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Linear Regression with TensorFlow </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constantes e variáveis não tem uma entrada *são chamadas source operations). A entrada e saída são vetores multidimensionais, chamados _tensores_. Assim como vetores do NumPy, tensores possuem tipo e formato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pega os dados California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "#Adiciona termo de bias x0 = 1\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "#Constroi o gráfo para método dos minímos quadrados (X^T.X)^-1.X^T.y\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O principal benefício da usar TensorFlow em vez do NumPy é que ele tem capacidade de rodar automaticamente em GPU (se instalado com suporte para GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementing Gradient Descent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar o dataset para melhor desempenho do gradiente\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Manually Computing the Gradients</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should be fairly self-explanatory, except for a few new elements:\n",
    "\n",
    "- The random_uniform() function creates a node in the graph that will generate a tensor containing random values, given its shape and value range, much like NumPy’s rand() function.\n",
    "- The assign() function creates a node that will assign a new value to a variable. In this case, it implements the BatchGradient Descent step θ(next step) = θ – η∇θMSE(θ).\n",
    "- The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints out the current Mean Squared Error (mse). You should see the MSE go down at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544262\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.5727805\n",
      "Epoch 300 MSE = 0.5585007\n",
      "Epoch 400 MSE = 0.54907\n",
      "Epoch 500 MSE = 0.542288\n",
      "Epoch 600 MSE = 0.53737885\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.5312425\n",
      "Epoch 900 MSE = 0.5293705\n"
     ]
    }
   ],
   "source": [
    "#Calculando o gradiente descendente manualmente\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Using autodiff </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código anterior funciona bem , mas ele requer o cálculo das derivadas matematicamente. No caso da regressão linear é fácil de fazer, mas quando se trabalha com deep learning começa a complicar. Você pode usar _symbolic_ _differentiation_ para automaticamente encontrar as equações para as derivadas parciais, mas o resultado pode não ser a maneira mais eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender, se for pego a função $f(x)=exp(exp(exp(x)))$ que possue a derivada $f'(x) = exp(x)*exp(exp(x))*exp(exp(exp(x)))$. Se o código for calcular exatamente como aparece não pode ser muito eficiente. A maneira mais eficiente seria calcular $exp(x)$, então $exp(exp(x))$ e então $exp(exp(exp(x)))$ e depois multiplicar. \n",
    "\n",
    "Pela maneira naive você teria que calcular exp(x) 9 vezes (f(x) e f'(x)). Com essa abordagem seriam necessários apenas 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E fica pior quando sua função é definida por um código arbritário. Você pode calcular as derivadas parciais do código a seguir? (nem tente!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felizmente, TensorFlow autodiff vem para ajudar! Ele pode automaticamente e eficientemente computar gradientes. Simplesmente troque os gradients = ... no código do gradiente descendente pela seguinte linha:\n",
    "\n",
    "    gradients = tf.gradients(mse, [theta])[0]\n",
    "   \n",
    "   e  o código funcionará corretamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544262\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.5727805\n",
      "Epoch 300 MSE = 0.5585007\n",
      "Epoch 400 MSE = 0.54907\n",
      "Epoch 500 MSE = 0.54228795\n",
      "Epoch 600 MSE = 0.5373789\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.5312425\n",
      "Epoch 900 MSE = 0.5293704\n"
     ]
    }
   ],
   "source": [
    "#Calculando o gradiente descendente com autodiff\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse,[theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função _gradients()_ pega a operação (no caso _mse_) e uma lista de variáveis (neste caso _theta_), e cria uma lista nde operações (uma por variável) para computar os gradientes da operação com relação a cada variável. Assim, o nó de gradientes calculará o vetor de gradiente do MSE com relação a theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem quatro abordagens para computar gradientes automaticamente. Eles estão sumarizados na Tabela 9-2. O TensorFlow utiliza _reverse-mode autodiff_, que é perfeito (eficientemente e acertivo) quando existem muitas entradas e poucas saídas, que frequentemente é o caso das redes neurais. Ele computa toda as derivadas parciais das saídas com relkação a todas as entradas em apenas $n_{\\text{outputs}} + 1$ gráfos transversais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagens/maincompgrap.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Using Optimizer </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de computar os gradientes para você, o TensorFlow provêm um número de ótimos otimizadores, incluindo Gradient Descent Optimizer. Você pode simplesmente trocar a linha do código anterior com gradients = ... e training_op = .. com o seguinte código:\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544262\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.5727805\n",
      "Epoch 300 MSE = 0.5585007\n",
      "Epoch 400 MSE = 0.54907\n",
      "Epoch 500 MSE = 0.54228795\n",
      "Epoch 600 MSE = 0.5373789\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.5312425\n",
      "Epoch 900 MSE = 0.5293704\n"
     ]
    }
   ],
   "source": [
    "#Calculando o gradiente descendente com Gradient Desceendent Optimizer\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você quiser utilizar um otimizador diferente, você pode apenas mudar uma linha. Por exemplo, você pode usar momentum optimizer (que freq converge mais rápido que o gradiente descendente), definindo um otimizador assim:\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544262\n",
      "Epoch 100 MSE = 0.52731586\n",
      "Epoch 200 MSE = 0.52441466\n",
      "Epoch 300 MSE = 0.52432835\n",
      "Epoch 400 MSE = 0.5243219\n",
      "Epoch 500 MSE = 0.524321\n",
      "Epoch 600 MSE = 0.52432066\n",
      "Epoch 700 MSE = 0.524321\n",
      "Epoch 800 MSE = 0.52432126\n",
      "Epoch 900 MSE = 0.524321\n"
     ]
    }
   ],
   "source": [
    "#Calculando o gradiente descendente com Momentum Optimizer\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feeding Data to the Training Algorithm </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maneira de trocar os valores de x e y em tempo de execução, é usando a função placeholder(). Esses nós são especiais porque eles não realizam computação, eles apenas emitem os dados que você entrega em tempo de execução. Se você não especificar os valores em tempo de execução para um placeholder, é levantado uma exceção.\n",
    "\n",
    "Você pode formalmente escolher o tamanho dos dados ou colocar _None_ para dimensões de qualquer tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32,shape=(None,3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: Na verdade, você pode alimentar (feed) a saída de qualquer operação, não apenas placeholders. Neste caso o TensorFlow não avalaria essas operações, ele usa os valores que foram colocados no feed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar o mini-batch Gradient Descent, nós precisamos trocar pouca coisa no código. Primeiro trocar a definição de x e y para placeholder()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "E definir o número de lotes e computar o número de lotes.\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "Na fase de execução, pegar os mini-batchs um por um, e prôver os valores de x e  y pelo feed_dict, quando avaliar um nó que depende de ambos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch\n",
    "\n",
    "#Calculando o gradiente descendente com Momentum Optimizer\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "#y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0714476 ],\n",
       "       [ 0.8462012 ],\n",
       "       [ 0.11558535],\n",
       "       [-0.26835832],\n",
       "       [ 0.32982782],\n",
       "       [ 0.00608358],\n",
       "       [ 0.07052915],\n",
       "       [-0.87988573],\n",
       "       [-0.8634251 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA: nós não precisamos passar x e y quando avaliar theta, pois ele não depende deles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving and Restoring Models </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para salvar modelos do TensorFlow apenas crie um nó Saver no fim da fase de construção (depois que todos os nós forem criados, e na fase de execução, chame o método sava() para salvar o modelo, passando a sessão e caminho do arquivo de checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 2.7544262\n",
      "Epoch 100 MSE = 0.632222\n",
      "Epoch 200 MSE = 0.5727805\n",
      "Epoch 300 MSE = 0.5585007\n",
      "Epoch 400 MSE = 0.54907\n",
      "Epoch 500 MSE = 0.54228795\n",
      "Epoch 600 MSE = 0.5373789\n",
      "Epoch 700 MSE = 0.533822\n",
      "Epoch 800 MSE = 0.5312425\n",
      "Epoch 900 MSE = 0.5293704\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Para recuperar o save\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por default, o Saver salva e recupera toda as variáveis com o seu próprimo nome, mas se for necessário um maior controle, você pode especificar quais variáveis salvar e recuperar, e qual nome você quer usar. Por exemplo, o código abaixo irá recuperar apenas a variável _theta_ com nome de _weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualizing the Graph and Training Curves Using TensorBoard </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós já fizemos um gráfo de computação que treina um modelo de regressão linear usando mini-batch gradient descent, e salvamos em intervalos regulares. Mas ainda para visualizar estavamos usando o print(). Existe uma maneira melhor de visualização chamada TensorBorard, se você adicionar algumas estatísticas de treinamento, ele irá mostrar visualizações interativas no seu browser (ex. learning curves). Você também provê a definição do grafo e ele irá mostrar uma interface para navegar. É muito últil para indetificar erros no gráfo e achar bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O primeiro passo é salvar o gráfo em um diretorio de log que o TensorBoard irá lêr\n",
    "#CUIDADO, é necessário criar diretorios de logm diferentes cada vez que executar seu programa\n",
    "#, caso contrário, ele irá dar um merge nas informações e bagunçar tudo\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        # not shown in the book\n",
    "    sess.run(init)                                                                # not shown\n",
    "\n",
    "    for epoch in range(n_epochs):                                                 # not shown\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                     # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.070016  ],\n",
       "       [ 0.8204561 ],\n",
       "       [ 0.1173173 ],\n",
       "       [-0.22739051],\n",
       "       [ 0.3113402 ],\n",
       "       [ 0.00353193],\n",
       "       [-0.01126994],\n",
       "       [-0.91643935],\n",
       "       [-0.8795008 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comando para iniciar o TensorBoard: tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: If you want to take a peek at the graph directly within Jupyter, you can use the show_graph() function available in the notebook\n",
    "for this chapter. It was originally written by A. Mordvintsev in his great deepdream tutorial notebook. Another option is to install\n",
    "E. Jang’s TensorFlow debugger tool which includes a Jupyter extension for graph visualization (and more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
